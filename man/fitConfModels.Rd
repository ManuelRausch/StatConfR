% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fitConfModels.R
\name{fitConfModels}
\alias{fitConfModels}
\title{Fit several static confidence models to multiple participants}
\usage{
fitConfModels(data, models = "all", var = "constant", .parallel = FALSE,
  n.cores = NULL)
}
\arguments{
\item{data}{a \code{data.frame} where each row is one trial, containing following
variables:
\itemize{
\item \code{condition} (not necessary; for different levels of stimulus quality,
should be a factor, otherwise it will be transformed to a factor with a
waring),
\item \code{rating} (discrete confidence judgments, should be given as factor;
otherwise will be transformed to factor with a warning),
\item \code{stimulus} (encoding the stimulus category in a binary choice task,
should be a factor with two levels, otherwise it will be transformed to
a factor with a warning),
\item \code{correct} (encoding whether the decision was correct; values in 0, 1),
\item \code{sbj} (giving the subject ID; the models given in the second argument are fitted for each
subject individually.
}}

\item{models}{\code{character} vector of models to be fit for each participant.
Implemented models: 'WEV', 'SDT', 'Noisy', 'PDA', and '2Chan'.
Alternatively, if \code{models="all"} (default), all implemented models will be fit.}

\item{var}{\code{character}. One of "constant" (default), "increasing", or "free" (will be
implemented), indicating how noise variances should be treated across conditions.
See Details for more information. Applies only to the models "SDT" and "WEV".}

\item{.parallel}{\code{logical}. Whether to parallelise the fitting over models and participant
(default: FALSE)}

\item{n.cores}{\code{integer}. Number of cores used for parallelization. If NULL (default), the available
number of cores -1 will be used.}
}
\value{
Gives data frame with rows for each model-participant combination and columns for the different parameters
as fitted result as well as additional information about the fit (\code{negLogLik} (for final parameters),
\code{k} (number of parameters), \code{N} (number of data rows), \code{BIC}, \code{AICc} and \code{AIC})
}
\description{
This function is a wrapper of the function \code{\link{fitConf}} (see
there for more information). It calls the function for every possible combination
of model in the \code{model} argument and participant in the \code{data}, respectively.
See the Details for more information about the parameters. All models are
described in full detail in Rausch et al. (2018).
}
\details{
The fitting involves a first grid search through an initial grid. Then the best 25 (\code{nAttempts})
parameter sets are chosen for an optimization, which is done with the Nelder-Mead algorithm implemented in \code{\link[stats]{optim}}.
\subsection{Mathematical description of models}{

This section contains a detailed mathematical description of all models
implemented in the package.

The computational models are all based on signal detection theory. Assume that
there are \eqn{k} different levels of difficulty manipulated in the
(and the levels are given by the \code{condition} column in the \code{data}) and that
the \code{stimulus} column indicated the identity of the true stimulus \eqn{S}
being either -1 or 1. Then, for each level of difficulty , a value for the
sensitivity \eqn{d_k} is fit. The models assume that the stimulus
generates normally distributed sensory evidence \eqn{x} with mean \eqn{Sd_k/2}
and variance \eqn{\sigma_k} (see below). The sensory evidence \eqn{x}
is compared to a decision threshold \eqn{\theta} to generate a choice response
\eqn{R}, which is 1, if \eqn{x} exceeds \eqn{\theta} and -1 else. (In the
output of the functions this will be A and B respectively.)
To generate confidence, the confidence variable \eqn{y} is compared to another
set of thresholds \eqn{c_{D,i}, D=A, B,  i=1,...,L-1}, depending on the
initial choice \eqn{D} to produce a \eqn{L}-step discrete confidence response.
The number of thresholds will be inferred by the number of steps in the
\code{rating} column of \code{data}.
The parameters common to all models are thus:
\itemize{
\item sensitivity parameters \eqn{d_1},...,\eqn{d_k} (\eqn{k}: number of difficulty levels)
\item choice threshold \eqn{\theta}
\item confidence threshold \eqn{c_{A,1}},...\eqn{c_{A,L-1}},\eqn{c_{B,1}},...
\eqn{c_{B,L-1}} (\eqn{L}: number of steps for confidence ratings)
}

How the confidence variable \eqn{y} is computed
varies from model to model. Following models are implemented:
\subsection{\strong{signal-detection theory (SDT)}}{

According to the signal-detection theory for confidence, the same sensory
evidence used to generate the response is used to generate confidence, i.e.
\eqn{y=x} and the confidence thresholds span from the left and
right side of the decision threshold \eqn{\theta}.
}

\subsection{\strong{Noisy signal-detection theory (Noisy)}}{

According to the noisy signal-detection theory, \eqn{y} is subject to
additional noise and assumed to be normally distributed around the initial
evidence value \eqn{x} with some standard deviation \eqn{\sigma}.
\eqn{\sigma} is an additional parameter fit to the model.
}

\subsection{\strong{weighted evidence and visibility model (WEV)}}{

WEV assumes that the observer combines evidence about choice-relevant features
of the stimulus with the strength of evidence about choice-irrelevant features
to generate confidence. Thus, the WEV model assumes that \eqn{y} is normally
distributed with a mean of \eqn{(1-w)x+wdR} and standard deviation \eqn{\sigma}.
The standard deviation quantifies the amount of unsystematic variability
contributing to confidence judgments but not to the identification judgments.
The parameter \eqn{w} represents the weight that is put on the choice-irrelevant
features in the confidence judgment. \eqn{w} and \eqn{\sigma} are fitted in
addition to the common parameters.
}

\subsection{\strong{Post-decisional accumulation model (PDA)}}{

PDA incorporates the idea of ongoing information accumulation after the
initial choice. The parameter \eqn{a} indicates the time of additional
accumulation. The confidence variable is normally distributed with mean
\eqn{x+Sda} and variance \eqn{a}.
For this model the parameter \eqn{a} is fitted in addition to the common
parameters.
}

\subsection{\strong{Two-channel model (2Chan)}}{

According to the two-channel model, \eqn{y} is sampled completely independent
from \eqn{x}. It is normally distributed with a mean of \eqn{ad} and variance
of 1 (again as it would scale with \eqn{a}). The additional parameter \eqn{a}
represents the amount of information available for the metacognitive judgment
relative to the type-I choice and can be smaller as well as greater than 1.
}

\subsection{\code{var} argument}{

The \code{var}argument will only apply to SDT and WEV models.
Depending on the \code{var} argument, the variance of the sensory evidence
\eqn{\sigma_k^2} will be treated as:
\itemize{
\item \code{constant} across conditions. In this case \eqn{\sigma_k^2 = 1} for all \eqn{k} since
it would scale with the sensitivity parameters
\item \code{increasing} across conditions. In this case
\eqn{\sigma_k^2 = 1+a(\frac{d_k}{2})^2} would increase with
sensitivity across conditions. \eqn{a} is a parameter that will be fit additionally.
}
}

}
}
\examples{
# 1. Generate data from two artificial participants
###      d1,  d2, d3, cA3, cA2, cA1,theta, cB1, cB2, cB3)
p1 <- c(0.2, 0.5, 1.5,-1.0,-0.5,-0.2,  0.1, 0.3, 0.6, 0.9)
p2 <- c(0.1, 0.7, 1.9,-0.8,-0.4,-0.1,  0.0, 0.1, 0.4, 0.8)
D <- rbind(p1[1:3], p2[1:3])
thresholds <- rbind(c(-Inf, p1[4:10], Inf),
                    c(-Inf, p2[4:10], Inf))
df <- expand.grid(n = 1:30, stimulus = c(-1, 1), condition=c(1,2, 3))
data <- data.frame()
for (i in 1:2) { #for each participant
  x <- rnorm(nrow(df), mean=df$stimulus*D[i, df$condition])
  response1 <- as.numeric(cut(x,breaks=thresholds[i,], include.lowest = TRUE))
  data <- rbind(data, cbind(df, x, response1, participant=i))
}
data$response <- ifelse(data$response1<=4, -1, 1)
data$correct <- as.numeric(data$response==data$stimulus)
data$rating <- as.factor(ifelse(data$response==-1, 5-data$response1, data$response1-4))
data$stimulus <- as.factor(data$stimulus)
data$condition <- as.factor(data$condition)
table(data[data$correct==1,c("participant", "condition")])/60
head(data)


# 2. Use fitting function
\dontrun{
  # Fitting takes very long to run and uses multiple cores with this
  # call:
  fitConfModels(data, models=c("SDT", "WEV", "Noisy"),
                .parallel=TRUE)
}


}
\references{
Rausch, M., Hellmann, S. & Zehetleitner, M. (2018). Confidence in
masked orientation judgments is informed by both evidence and visibility.
\emph{Atten Percept Psychophys} 80, 134â€“154. doi: 10.3758/s13414-017-1431-5
}
\author{
Sebastian Hellmann, \email{sebastian.hellmann@ku.de}
}
